<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SAFE: Scalable Failure Estimation for Vision-Language-Action Models">
  <meta name="keywords"
    content="SAFE, Vision-Language-Action Model, VLA, Failure Detection, Failure Estimation, Robotics, AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SAFE: Scalable Failure Estimation for Vision-Language-Action Models</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TEBZD9Q5QS"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TEBZD9Q5QS');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://vla-safe.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <!-- <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Related Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://concept-fusion.github.io">
              ConceptFusion
            </a>
            <a class="navbar-item" href="https://gradslam.github.io">
              GradSLAM
            </a>
            <a class="navbar-item" href="https://sayplan.github.io/">
              SayPlan
            </a>
          </div>
        </div> -->
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">SAFE: Scalable Failure Estimation for Vision-Language-Action Models</h1>
            <h2 class="title is-6 publlication-title">In Submission</h2>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="https://georgegu1997.github.io/"><i>Qiao Gu</i></a> <sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=rG90YVAAAAAJ&hl=zh-CN"><i>Yuanliang Ju</i></a> <sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://owensun2004.github.io/"><i>Shengxiang Sun</i></a> <sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.gilitschenski.org/igor/"><i>Igor Gilitschenski</i></a> <sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://harukins.github.io/"><i>Haruki Nishimura</i></a> <sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://mashaitkina.weebly.com/"><i>Masha Itkina</i></a> <sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.toronto.edu/~florian/"><i>Florian Shkurti</i></a> <sup>1,2,3</sup>,
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>University of Toronto (UofT),</span>
              <span class="author-block"><sup>2</sup>UofT Robotics Institute</span>
              <span class="author-block"><sup>3</sup>Vector Institute</span>
              <span class="author-block"><sup>4</sup>Toyota Research Institute (TRI)</span>
              <br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal" disabled="true">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal" disabled="true">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal" disabled="true">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal" disabled="true">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop is-full-width">
      <div class="columns is-centered">
        <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/safe-teaser.mp4" type="video/mp4">
        </video>
      </div>
      <div class="columns is-centered">
        <h2 class="subtitle has-text-centered">
            We introduce the <b>multitask failure detection</b> problem for VLA models, and propose <b><span class="coolname">SAFE</span></b>, a failure detector that can detect failures for unseen tasks zero-shot and achieve state-of-the-art performance.
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Explainer Video</h2>
          <div class="publication-video">
            <iframe width="100%" height="473" src="https://www.youtube.com/embed/dWuZyeiOXyM?si=14CuGsvIrmxstIFK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose <b><span class="coolname">SAFE</span></b>, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design <span class="coolname">SAFE</span> to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. <span class="coolname">SAFE</span> is trained on both successful and failed rollouts, and is evaluated on unseen tasks. <span class="coolname">SAFE</span> is compatible with different policy architectures. We test it on OpenVLA, &pi;<sub>0</sub>, and &pi;<sub>0</sub>-FAST in both simulated and real-world environments extensively. We compare <span class="coolname">SAFE</span> with diverse baselines and show that <span class="coolname">SAFE</span> achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. 
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">VLA Latent Feature Analysis</h2>
          <div class="content has-text-justified">
            <p>
              We find that the <b>VLA's internal features capture high-level information about task success and failure, and such information is general across different tasks</b>. As shown in the figure below, when a VLA is failing, even though from different tasks, the features fall in the same <i>failure zone</i>. This motivates <span class="coolname">SAFE</span>, an efficient multitask failure detector that is based on VLA internal features and can generalize to unseen tasks. 
            </p>
            <!-- <p>
              In the figure below, we visualize the latent features of <a href="https://github.com/Physical-Intelligence/openpi">&pi;<sub>0</sub>-FAST</a> on the <a href="https://github.com/Lifelong-Robot-Learning/LIBERO">LIBERO-10</a> benchmark.
              In (a), features from successful rollouts are shown in blue and those from failed ones are shown in blue-red color gradient. 
              In (b), we visualizes the same set of t-SNE features, colored by task ID. 
              In (c), we show two example rollouts over time and mark their corresponding projected features in (a) and (b).
            </p> -->
            <img src="./static/images/vla-feature.png" />
          </div>
          <br />
        </div>
      </div>


      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">SAFE: Multitask Failure Detector for VLA Models</h2>
            <div class="content has-text-justified">
              <p>
                Based on the above observation, we propose <span class="coolname">SAFE</span>, a failure detector that learns from VLA internal features and predicts a single scalar indicating the likelihood of task failure. <span class="coolname">SAFE</span> has 3 main components:
              </p>
              <ul>
                <li><b>Feature Extraction</b>: <span class="coolname">SAFE</span> extracts the latent feature from the last layer of a VLA model. In experiments, we ablate different ways of extracting features and aggregate them into a single feature vector.</li>
                <li><b>Learning Failure Detector</b>: <span class="coolname">SAFE</span> sequentially processes the latent feature and predicts a failure score, using an MLP or an LSTM backbone. These models are of 1 or 2 layers to reduce overfitting and improve generalization.</li>
                <li><b>Calibration and Deployment</b>: <span class="coolname">SAFE</span> determines a time-varying threshold using functional conformal prediction (CP) on a hold-out calibration set. If the predicted score exceeds the threshold during testing, <span class="coolname">SAFE</span> confidently detects a failure. </li>
              </ul>
              <img src="./static/images/safe-method.png" />
            </div>

            <h2 class="title is-3">Experiments and Results</h2>
            <div class="content has-text-justified">
              <p>
                We evaluate the following diverse baselines. All the baselines use the same conformal prediction framework as <span class="coolname">SAFE</span> to determine the time-varying threshold.
              </p>
              <ul>
                <li><b>Token Uncertainty</b>: Failure scores are computed based on token-wise uncertainty (probability and entropy). </li>
                <li><b>Embedding Distribution</b>: Failure scores are computed based on the embedding distances to the calibration distribution. </li>
                <li><b>Sample Consistency</b>: Multiple actions are sampled and failure scores are the inconsistency among the samples. </li>
                <li><b>Action Consistency</b>: We adopt <a href="https://arxiv.org/abs/2410.04640">STAC scores</a> and also STAC-single that only uses a single sample per timestep. </li>
              </ul>
              <p>
                We conduct experiments on <a href="https://github.com/openvla/openvla">OpenVLA</a>, <a href="https://github.com/Physical-Intelligence/openpi">&pi;<sub>0</sub> and &pi;<sub>0</sub>-FAST</a> VLA models on <a href="https://github.com/Lifelong-Robot-Learning/LIBERO">LIBERO</a>, <a href="https://github.com/simpler-env/SimplerEnv">SimplerEnv</a> benchmarks and a real-world Franka robot. 
              </p>
            </div>

            <h3 class="is-3">How well do failure detectors distinguish failures from successes?</h3>
            <div class="content has-text-justified">
              <p>
                Following the LLM uncertainty quantification literature, we report the ROC curve (ROC-AUC) metrics in the following figure, which are computed based on the max predicted failure score in each rollout. ROC-AUC averages the performance over all possible thresholds, which reflects the overall failure detection performance regardless of threshold selection. 
              </p>
              <img src="./static/images/safe-results-roc.png" />
            </div>

            <h3 class="is-3">How do detection accuracy and detection time trade off using functional CP?</h3>
            <div class="content has-text-justified">
              <img src="./static/images/safe-acc-tdet.png" />
            </div>

            <!-- <h2 class="title is-3">Experiments</h2>
            <p>
              Egocentric videos capture frequent human-object interaction and thus contain a huge amount of dynamic motion with challenging occlusions. <span class="coolname">EgoLifter</span> is designed to provide useful scene understanding from egocentric data by extracting hundreds of different objects while being robust to sparse and rapid dynamics.
            </p>
            <p>
              To demonstrate this, we compare the following variants in our experiments: 
            </p>
            <ul>
              <li><span class="coolname">EgoLifter</span>: Our proposed full method</li>
              <li><span class="coolname">EgoLifter</span>-Static: a baseline with the transient prediction network disabled. A vanilla static 3DGS is learned to reconstruct the scene. </li>
              <li><span class="coolname">EgoLifter</span>-Dynamic: a baseline using a dynamic variant of 3DGS, instead of the transient prediction network to handle the dynamics in the scene. </li>
              <li><a href="https://arxiv.org/abs/2312.00732">Gaussian Grouping</a>: a concurrent work that also learns instance features in 3DGS. However, Gaussian Grouping uses a video tracker to solve instance identities rather than contrastive learning. </li>
            </ul>
            <p>
              Here are the qualitative results of <span class="coolname">EgoLifter</span> on the <a href="https://www.projectaria.com/datasets/adt/">Aria Digital Twin</a> (ADT) dataset. Quantitative evaluation results and more analysis can be found in the paper. Note that baseline puts ghostly floaters on the region of transient objects, but <span class="coolname">EgoLifter</span> filters them out and gives a cleaner reconstruction of both RGB images and feature maps. 
            </p>
            <div class="qual-video-adt">
              <video poster="" controls muted loop playsinline height="100%" width="100%">
                <source src="static/videos/compare_3_faster-meal132-seen.mp4" type="video/mp4">
              </video>
            </div>
            <p>
              Here is the qualitative comparison with Gaussian Grouping. Note that <span class="coolname">EgoLifter</span> has a cleaner feature map probably because our contrastive loss helps learn more cohesive identity features than the classification loss used in Gaussian Grouping. 
            </p>
            <div class="qual-video-adt">
              <video poster="" controls muted loop playsinline height="100%" width="75%">
                <source src="static/videos/compare_gg_less-meal132-seen.mp4" type="video/mp4">
              </video>
            </div> -->
          </div>
        </div>
      </div>

    </div>
  </section>


  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">More Qualitative Results</h2>
            <p>
              Here are more qualitative results of <span class="coolname">EgoLifter</span> on the <a href="https://ego-exo4d-data.org/">Ego-Exo4D</a> dataset and <a href="https://www.projectaria.com/datasets/aea/">Aria Everyday Activities</a> dataset.
            </p>
          </div>
        </div>
      </div>
      
      <div class="columns is-centered">

        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <video poster="" id="snoopy" controls muted loop playsinline height="100%" width="100%">
                <source src="static/videos/compare_path-egoexo-cmu_bike16-trim.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <video poster="" id="snoopy" controls muted loop playsinline height="100%" width="100%">
                <source src="static/videos/compare_path-sfu_cooking025.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

      </div>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <video poster="" id="snoopy" controls muted loop playsinline height="100%" width="100%">
              <source src="static/videos/compare_path-aea-loc2.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <video poster="" id="snoopy" controls muted loop playsinline height="100%" width="100%">
                <source src="static/videos/compare_path-aea-loc4.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

      </div>

    </div>
  </section> -->


  <!-- <section class="section" id="concurrent work">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Concurrent work</h2>

          <div class="content has-text-justified">
            <p>
              Given the pace of AI research these days, it is extremely challenging to keep up with all of the work
              around foundation models and open-set perception. We list below a few key approaches that we have come
              across after finalizing the ConceptGraphs system. If we may have inadvertently missed out on key
              concurrent work, please reach out to us over email (or better, open a pull request on <a
                href="https://github.com/concept-graphs/concept-graphs.github.io">our GitHub page</a>).
            </p>
            <p>
              <a href="https://openreview.net/forum?id=gVBvtRqU1_">OVIR-3D</a> is an open-vocabulary 3D instance-level
              mapping system that reconstructs an objects from RGB-D images and known poses. Each object is additionally
              assigned a CLIP embedding for text-query-based retrieval.
            </p>
            <p>
              <a href="https://openmask3d.github.io/">OpenMask3D</a> performs 3D instance segmentation (on pointcloud
              data) based on open-vocabulary queries (specified as text).
            </p>
            <p>
              <a href="https://openreview.net/forum?id=cjEI5qXoT0">OVSG</a> reconstructs open-vocabulary 3D scene graphs
              using OVIR-3D and a graph network encoder.
            </p>
            <p>
              <a href="https://sayplan.github.io/">SayPlan</a> demonstrates an efficient planning mechanism using LLMs
              and 3D Scene Graphs (assumed available).
            </p>
          </div>
        </div>
      </div>

    </div>
  </section> -->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{gu2025safe,
  author    = {Gu, Qiao and Lv, Zhaoyang and Frost, Duncan and Green, Simon and Straub, Julian and Sweeney, Chris},
  title     = {SAFE: Scalable Failure Estimation for Vision-Language-Action Models},
  journal   = {Preprint},
  year      = {2025},
} </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./assets/pdf/_ECCV2024_EgoLifter.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/concept-graphs" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
          <img alt="Creative Commons License" style="border-width:0"
            src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website adapted from the Nerfies templates, which is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a
                href="https://github.com/concept-fusion/concept-fusion.github.io">source code</a> of this website,
              we just ask that you link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies
                source code</a> in the footer.
              Please remember to remove the analytics code included in the header of the website which you do not want
              on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>